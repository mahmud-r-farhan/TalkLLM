# ðŸ§  TalkLLM - A Local LLM Chat App using WebLLM

Welcome to **TalkLLM**, a lightweight and modern chat interface powered by [MLC AI WebLLM](https://mlc.ai/web-llm/). This app allows you to run a **Large Language Model (LLM) directly in your browser** with **no backend or server required**.

Built using **React**, this app uses the **Llama 3 model** locally for a private and blazing-fast AI assistant experience.

---

## ðŸš€ Features

- âœ… Local-first LLM (no internet API calls)
- ðŸ’¬ Seamless chat experience with context
- ðŸ“¦ Runs directly in the browser using `@mlc-ai/web-llm`
- ðŸ›¡ï¸ No data sent to servers
- âš¡ Clean, modern UI with loading indicators and error handling

---

## ðŸ“¸ Screenshot

![screenshot](./screenshot.png)

---

## ðŸ› ï¸ Getting Started


### Prerequisites

Before running this project, make sure your system meets the following requirements:

#### ðŸ–¥ï¸ System Requirements

- âœ… A browser that supports **WebGPU**
  - Recommended: **Latest Chrome**, **Edge**, or **Safari** 
  - [Check if your browser supports WebGPU](https://webgpu.report/)

- âœ… A device with **WebAssembly (Wasm)** support (most modern devices have this by default)

#### âš™ï¸ Development Environment

- Node.js (v16 or above recommended)
- npm or yarn


### Installation

```bash
https://github.com/mahmud-r-farhan/TalkLLM.git
cd TalkLLM
npm install

```

### Running Locally

```bash
npm run dev

```

The app will open at `http://localhost:5173`.

----------

## ðŸ§  Model

This app uses:

```
Llama-3.1-8B-Instruct-q4f32_1-MLC

```

The model is initialized locally using:

```js
webllm.CreateMLCEngine(modelName, { initProgressCallback });

```

No server, no data sharing â€” everything runs in the browser.

----------

## ðŸ§¾ Project Structure

```
src/
â”œâ”€â”€ App.jsx          # Main chat component
â”œâ”€â”€ ui.scss          # Styling
```

----------

## âš™ï¸ Customization

You can easily switch to another supported model by updating this line:

```js
const selectedModel = "Llama-3.1-8B-Instruct-q4f32_1-MLC";

```

Refer to [MLC model catalog](https://mlc.ai/web-llm/docs/#model-catalog) for available models.

----------

## ðŸ“¦ Dependencies

-   React
    
-   @mlc-ai/web-llm
    
-   Sass (`.scss` support for UI styling)
    

Install Sass if needed:

```bash
npm install sass

```
or,

```bash
npm i sass-embedded

```

----------

## ðŸ¤ Contributing

Pull requests are welcome! Feel free to open issues or submit improvements.

----------

## Credits

-   [MLC AI](https://mlc.ai/web-llm/) for the awesome WebLLM engine

----------

> ðŸ§ª Tip: You can use this as a local AI assistant app without relying on OpenAI or any third-party API.
